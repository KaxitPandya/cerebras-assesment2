{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4f7ba8ea",
      "metadata": {
        "id": "4f7ba8ea"
      },
      "source": [
        "# vLLM MoE Expert Logging - Complete Solution\n",
        "\n",
        "This notebook implements MoE expert logging for vLLM using the Qwen1.5-MoE-A2.7B-Chat model.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with GPU runtime (T4 or better)\n",
        "- ~15GB GPU memory for the model\n",
        "\n",
        "**Setup:** Runtime → Change runtime type → T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89f4e96",
      "metadata": {
        "id": "e89f4e96"
      },
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "⚠️ **IMPORTANT**: After running the install cell below, you MUST:\n",
        "1. Go to `Runtime` → `Restart runtime`\n",
        "2. After restart, **SKIP the install cell** and continue from \"Verify installation\"\n",
        "\n",
        "This is required because vLLM updates numpy, which requires a runtime restart to take effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "56be1817",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56be1817",
        "outputId": "f0c2b8bf-5f13-41c4-84f2-fedb6bd9a6b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m779.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.2/33.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m914.4/914.4 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.5.1 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m============================================================\n",
            "IMPORTANT: Restart runtime now!\n",
            "Go to: Runtime -> Restart runtime\n",
            "Then SKIP this cell and continue from the next cell\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Install vLLM and dependencies\n",
        "# Note: After this cell, you MUST restart the runtime (Runtime -> Restart runtime)\n",
        "# Then skip this cell and continue from the next one\n",
        "\n",
        "!pip install vllm==0.6.6.post1 datasets matplotlib --quiet\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"IMPORTANT: Restart runtime now!\")\n",
        "print(\"Go to: Runtime -> Restart runtime\")\n",
        "print(\"Then SKIP this cell and continue from the next cell\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c19296d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c19296d1",
        "outputId": "be56ac0f-ce35-4808-e092-b7f4ab32474f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM version: 0.6.6.post1\n",
            "PyTorch version: 2.5.1+cu124\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Verify installation\n",
        "import vllm\n",
        "import torch\n",
        "print(f\"vLLM version: {vllm.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b01b2f29",
      "metadata": {
        "id": "b01b2f29"
      },
      "source": [
        "## Step 2: Create the MoE Logger Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f158ddb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f158ddb7",
        "outputId": "73d56fb0-3645-4dcc-ad43-9c53ba67b51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting moe_logger.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile moe_logger.py\n",
        "\"\"\"\n",
        "MoE Expert Routing Logger for vLLM\n",
        "\n",
        "This module provides a singleton logger that records MoE expert routing decisions\n",
        "to a JSONL file when enabled via the VLLM_LOG_MOE environment variable.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import vllm\n",
        "from typing import Optional, List\n",
        "from threading import Lock\n",
        "\n",
        "\n",
        "class MoELogger:\n",
        "    \"\"\"Singleton logger for MoE expert routing.\"\"\"\n",
        "\n",
        "    _instance: Optional['MoELogger'] = None\n",
        "    _lock = Lock()\n",
        "\n",
        "    def __new__(cls):\n",
        "        if cls._instance is None:\n",
        "            with cls._lock:\n",
        "                if cls._instance is None:\n",
        "                    cls._instance = super().__new__(cls)\n",
        "                    cls._instance._initialized = False\n",
        "        return cls._instance\n",
        "\n",
        "    def __init__(self):\n",
        "        if self._initialized:\n",
        "            return\n",
        "\n",
        "        self._initialized = True\n",
        "        self.log_path = os.environ.get('VLLM_LOG_MOE', None)\n",
        "        self.enabled = self.log_path is not None\n",
        "        self.file_handle = None\n",
        "        self.header_written = False\n",
        "        self.token_counter = 0\n",
        "        self.request_counter = 0\n",
        "        self.current_request_id = \"r0\"\n",
        "\n",
        "        # Configuration - OLMoE-1B-7B has 64 experts, top_k=8\n",
        "        self.layers_to_log = [0]  # Log only layer 0 by default\n",
        "        self.top_k = 8  # OLMoE uses top_k=8\n",
        "        self.num_experts = 64  # OLMoE has 64 experts\n",
        "\n",
        "        if self.enabled:\n",
        "            self._open_file()\n",
        "            print(f\"[MoE Logger] Initialized. Logging layer 0 to {self.log_path}\")\n",
        "\n",
        "    def _open_file(self):\n",
        "        \"\"\"Open log file and write header.\"\"\"\n",
        "        try:\n",
        "            self.file_handle = open(self.log_path, 'w')\n",
        "            self._write_header()\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not open MoE log file: {e}\")\n",
        "            self.enabled = False\n",
        "\n",
        "    def _write_header(self):\n",
        "        \"\"\"Write the metadata header line.\"\"\"\n",
        "        if self.header_written:\n",
        "            return\n",
        "\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.cuda.get_device_name(0)\n",
        "\n",
        "        header = {\n",
        "            \"type\": \"meta\",\n",
        "            \"model_id\": os.environ.get('VLLM_MODEL_ID', 'allenai/OLMoE-1B-7B-0924'),\n",
        "            \"vllm_version\": vllm.__version__,\n",
        "            \"torch_version\": torch.__version__,\n",
        "            \"device\": device,\n",
        "            \"seed\": 1234,\n",
        "            \"layers_logged\": self.layers_to_log,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"num_experts\": self.num_experts\n",
        "        }\n",
        "\n",
        "        self.file_handle.write(json.dumps(header) + '\\n')\n",
        "        self.file_handle.flush()\n",
        "        self.header_written = True\n",
        "\n",
        "    def log_routing(self, layer_idx: int, topk_ids: torch.Tensor, topk_weights: torch.Tensor):\n",
        "        \"\"\"Log routing decision for a batch of tokens.\"\"\"\n",
        "        if not self.enabled or layer_idx not in self.layers_to_log:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # topk_ids shape: [num_tokens, top_k]\n",
        "            # topk_weights shape: [num_tokens, top_k]\n",
        "            ids = topk_ids.detach().cpu().tolist()\n",
        "            weights = topk_weights.detach().cpu().tolist()\n",
        "\n",
        "            for i, (token_ids, token_weights) in enumerate(zip(ids, weights)):\n",
        "                record = {\n",
        "                    \"type\": \"route\",\n",
        "                    \"req_id\": self.current_request_id,\n",
        "                    \"token_idx\": self.token_counter,\n",
        "                    \"layer\": layer_idx,\n",
        "                    \"topk_ids\": token_ids,\n",
        "                    \"topk_weights\": [round(w, 4) for w in token_weights]\n",
        "                }\n",
        "                self.file_handle.write(json.dumps(record) + '\\n')\n",
        "                self.token_counter += 1\n",
        "\n",
        "            self.file_handle.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"[MoE Logger] Error logging: {e}\")\n",
        "\n",
        "    def new_request(self):\n",
        "        \"\"\"Signal start of a new request.\"\"\"\n",
        "        self.request_counter += 1\n",
        "        self.current_request_id = f\"r{self.request_counter}\"\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the log file.\"\"\"\n",
        "        if self.file_handle:\n",
        "            self.file_handle.close()\n",
        "            self.file_handle = None\n",
        "            print(f\"[MoE Logger] Closed. Logged {self.token_counter} token routings.\")\n",
        "\n",
        "\n",
        "def get_moe_logger() -> MoELogger:\n",
        "    \"\"\"Get the singleton MoE logger instance.\"\"\"\n",
        "    return MoELogger()\n",
        "\n",
        "\n",
        "def reset_moe_logger():\n",
        "    \"\"\"Reset the singleton for testing purposes.\"\"\"\n",
        "    MoELogger._instance = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd31097",
      "metadata": {
        "id": "3fd31097"
      },
      "source": [
        "## Step 3: Create the vLLM MoE Patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4b16448e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b16448e",
        "outputId": "dffd927e-5a8c-42ef-8ec4-2e02a67becc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vllm_moe_patch.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile vllm_moe_patch.py\n",
        "\"\"\"\n",
        "vLLM MoE Logging Patch\n",
        "\n",
        "This module patches vLLM's FusedMoE layer to enable expert routing logging.\n",
        "Import this module BEFORE creating the LLM instance to apply the patch.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from moe_logger import get_moe_logger\n",
        "\n",
        "# Only apply patch if logging is enabled\n",
        "if os.environ.get('VLLM_LOG_MOE'):\n",
        "    print(f\"MoE logging enabled, output: {os.environ.get('VLLM_LOG_MOE')}\")\n",
        "\n",
        "    try:\n",
        "        # Import the FusedMoE class\n",
        "        from vllm.model_executor.layers.fused_moe import FusedMoE\n",
        "\n",
        "        # Store original forward method\n",
        "        _original_forward = FusedMoE.forward\n",
        "\n",
        "        # Layer counter for tracking which layer we're in\n",
        "        _layer_call_count = [0]\n",
        "        _num_layers = [24]  # Qwen1.5-MoE has 24 layers\n",
        "\n",
        "        def patched_forward(self, hidden_states: torch.Tensor, *args, **kwargs):\n",
        "            \"\"\"Patched forward that logs routing decisions.\"\"\"\n",
        "            # Determine current layer index\n",
        "            layer_idx = _layer_call_count[0] % _num_layers[0]\n",
        "            _layer_call_count[0] += 1\n",
        "\n",
        "            # Get router logits to compute routing before calling original\n",
        "            logger = get_moe_logger()\n",
        "\n",
        "            if logger.enabled and layer_idx in logger.layers_to_log:\n",
        "                # Compute router logits manually for logging\n",
        "                try:\n",
        "                    router_logits = self.gate(hidden_states)\n",
        "\n",
        "                    # Compute top-k routing\n",
        "                    routing_weights = torch.softmax(router_logits, dim=-1)\n",
        "                    topk_weights, topk_ids = torch.topk(routing_weights, k=self.top_k, dim=-1)\n",
        "\n",
        "                    # Normalize weights\n",
        "                    topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
        "\n",
        "                    # Log the routing\n",
        "                    logger.log_routing(layer_idx, topk_ids, topk_weights)\n",
        "                except Exception as e:\n",
        "                    pass  # Silent fail\n",
        "\n",
        "            # Call original forward\n",
        "            return _original_forward(self, hidden_states, *args, **kwargs)\n",
        "\n",
        "        # Apply patch\n",
        "        FusedMoE.forward = patched_forward\n",
        "        print(\"MoE logging patch applied successfully!\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"Warning: Could not apply MoE patch: {e}\")\n",
        "        print(\"Falling back to alternative patching method...\")\n",
        "\n",
        "        # Alternative: Patch at the model level\n",
        "        try:\n",
        "            from vllm.model_executor.models.qwen2_moe import Qwen2MoeSparseMoeBlock\n",
        "\n",
        "            _original_moe_forward = Qwen2MoeSparseMoeBlock.forward\n",
        "            _layer_counter = [0]\n",
        "\n",
        "            def patched_moe_forward(self, hidden_states: torch.Tensor):\n",
        "                \"\"\"Patched MoE block forward.\"\"\"\n",
        "                layer_idx = _layer_counter[0] % 24\n",
        "                _layer_counter[0] += 1\n",
        "\n",
        "                logger = get_moe_logger()\n",
        "\n",
        "                if logger.enabled and layer_idx in logger.layers_to_log:\n",
        "                    try:\n",
        "                        # Get router logits\n",
        "                        router_logits = self.gate(hidden_states)\n",
        "                        routing_weights = torch.softmax(router_logits, dim=-1)\n",
        "                        topk_weights, topk_ids = torch.topk(\n",
        "                            routing_weights, k=self.top_k, dim=-1\n",
        "                        )\n",
        "                        topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)\n",
        "                        logger.log_routing(layer_idx, topk_ids, topk_weights)\n",
        "                    except Exception as e:\n",
        "                        pass\n",
        "\n",
        "                return _original_moe_forward(self, hidden_states)\n",
        "\n",
        "            Qwen2MoeSparseMoeBlock.forward = patched_moe_forward\n",
        "            print(\"MoE logging patch (alternative) applied successfully!\")\n",
        "\n",
        "        except ImportError as e2:\n",
        "            print(f\"Warning: Alternative patching also failed: {e2}\")\n",
        "else:\n",
        "    print(\"MoE logging disabled (VLLM_LOG_MOE not set)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f7c6d4",
      "metadata": {
        "id": "d3f7c6d4"
      },
      "source": [
        "## Step 4: Create Prompts from GSM8K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ed919ffa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed919ffa",
        "outputId": "1536d743-210d-4fa7-ea7c-77679c273e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting make_prompts.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile make_prompts.py\n",
        "\"\"\"Generate prompts from GSM8K dataset.\"\"\"\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load GSM8K test split (MIT licensed)\n",
        "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "\n",
        "# Get first 25 questions\n",
        "prompts = [ex[\"question\"] for ex in ds.select(range(25))]\n",
        "\n",
        "# Save with delimiter\n",
        "with open(\"prompts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\\n---\\n\\n\".join(prompts))\n",
        "\n",
        "print(f\"Saved {len(prompts)} prompts to prompts.txt\")\n",
        "print(f\"First prompt: {prompts[0][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "2eb83590",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eb83590",
        "outputId": "5b3fcfae-5142-462d-dd19-ddca0cd20202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 25 prompts to prompts.txt\n",
            "First prompt: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n"
          ]
        }
      ],
      "source": [
        "# Run the prompt generation\n",
        "!python make_prompts.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d2aec1c",
      "metadata": {
        "id": "6d2aec1c"
      },
      "source": [
        "## Step 5: Create the Main Generation Script"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat moe_routes.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dAX1kaoOE84",
        "outputId": "d2e111ec-04bd-45f0-dabc-3a76326c385c"
      },
      "id": "_dAX1kaoOE84",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"type\": \"meta\", \"model_id\": \"allenai/OLMoE-1B-7B-0924\", \"vllm_version\": \"0.6.6.post1\", \"torch_version\": \"2.5.1+cu124\", \"device\": \"Tesla T4\", \"seed\": 1234, \"layers_logged\": [0], \"top_k\": 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "7fa2c853",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fa2c853",
        "outputId": "9bccf171-3153-427d-c8fe-22556e298548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_generate.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_generate.py\n",
        "\"\"\"\n",
        "run_generate.py - Generate text using vLLM with MoE expert logging support\n",
        "\n",
        "Usage:\n",
        "  python run_generate.py                          # Without logging\n",
        "  VLLM_LOG_MOE=moe_routes.jsonl python run_generate.py  # With logging\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Clear any existing GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# Apply MoE logging patch BEFORE importing vLLM\n",
        "if os.environ.get('VLLM_LOG_MOE'):\n",
        "    import vllm_moe_patch\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# OLMoE-1B-7B: A tiny MoE model that fits in T4!\n",
        "# - Only 7B total parameters (1B active per token)\n",
        "# - 64 experts, top_k=8\n",
        "# - ~14GB in float16, fits in T4's 15GB\n",
        "MODEL_ID = \"allenai/OLMoE-1B-7B-0924\"\n",
        "\n",
        "# Set model ID for logging\n",
        "os.environ['VLLM_MODEL_ID'] = MODEL_ID\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(1234)\n",
        "\n",
        "# Load prompts\n",
        "print(\"Loading prompts...\")\n",
        "prompts = open(\"prompts.txt\", encoding=\"utf-8\").read().split(\"\\n\\n---\\n\\n\")\n",
        "print(f\"Loaded {len(prompts)} prompts\")\n",
        "\n",
        "# Create sampling parameters\n",
        "sp = SamplingParams(temperature=0.0, max_tokens=128, seed=1234)\n",
        "\n",
        "# Initialize LLM\n",
        "print(f\"Initializing vLLM with {MODEL_ID}...\")\n",
        "llm = LLM(\n",
        "    model=MODEL_ID,\n",
        "    max_model_len=512,  # Small context to save memory\n",
        "    trust_remote_code=True,\n",
        "    gpu_memory_utilization=0.98, # Increased to 0.98\n",
        "    enforce_eager=True,  # Disable CUDA graphs\n",
        "    dtype=\"half\", # Changed to half for T4 compatibility\n",
        ")\n",
        "\n",
        "# Generate\n",
        "print(\"Generating...\")\n",
        "t0 = time.time()\n",
        "outs = llm.generate(prompts, sp)\n",
        "t1 = time.time()\n",
        "\n",
        "elapsed = t1 - t0\n",
        "total_tokens = sum(len(o.outputs[0].token_ids) for o in outs)\n",
        "\n",
        "print(f\"\\nGeneration complete!\")\n",
        "print(f\"Time: {elapsed:.2f}s\")\n",
        "print(f\"Tokens: {total_tokens}\")\n",
        "print(f\"Tokens/sec: {total_tokens/elapsed:.2f}\")\n",
        "\n",
        "# Save timing results\n",
        "timing_file = \"timing.json\"\n",
        "if os.path.exists(timing_file):\n",
        "    with open(timing_file, 'r') as f:\n",
        "        timing_data = json.load(f)\n",
        "else:\n",
        "    timing_data = {}\n",
        "\n",
        "# Determine if logging was enabled\n",
        "log_key = \"log\" if os.environ.get('VLLM_LOG_MOE') else \"no_log\"\n",
        "timing_data[log_key] = {\n",
        "    \"wall_time_sec\": elapsed,\n",
        "    \"tokens_generated\": total_tokens,\n",
        "    \"tokens_per_sec\": total_tokens / elapsed\n",
        "}\n",
        "\n",
        "with open(timing_file, 'w') as f:\n",
        "    json.dump(timing_data, f, indent=2)\n",
        "\n",
        "print(f\"Timing data saved to {timing_file}\")\n",
        "\n",
        "# Close logger if enabled\n",
        "if os.environ.get('VLLM_LOG_MOE'):\n",
        "    from moe_logger import get_moe_logger\n",
        "    get_moe_logger().close()\n",
        "    print(f\"MoE routing log saved to {os.environ.get('VLLM_LOG_MOE')}\")\n",
        "\n",
        "# Print sample outputs\n",
        "print(\"\\n=== Sample Outputs ===\")\n",
        "for i, out in enumerate(outs[:3]):\n",
        "    print(f\"\\nPrompt {i+1}: {prompts[i][:80]}...\")\n",
        "    print(f\"Output: {out.outputs[0].text[:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e61052f",
      "metadata": {
        "id": "4e61052f"
      },
      "source": [
        "## Step 6: Create the Plotting Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "d6de08f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6de08f7",
        "outputId": "95b0eeff-7acf-41a5-a9f2-39921bb75bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting plot_expert_histogram.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile plot_expert_histogram.py\n",
        "\"\"\"\n",
        "plot_expert_histogram.py - Generate expert usage histogram from MoE routing log\n",
        "\n",
        "Reads moe_routes.jsonl and produces expert_hist.png with analysis.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import sys\n",
        "\n",
        "\n",
        "def load_routing_data(jsonl_path: str):\n",
        "    \"\"\"Load routing data from JSONL file.\"\"\"\n",
        "    metadata = None\n",
        "    routes = []\n",
        "\n",
        "    with open(jsonl_path, 'r') as f:\n",
        "        for line in f:\n",
        "            record = json.loads(line.strip())\n",
        "            if record['type'] == 'meta':\n",
        "                metadata = record\n",
        "            elif record['type'] == 'route':\n",
        "                routes.append(record)\n",
        "\n",
        "    return metadata, routes\n",
        "\n",
        "\n",
        "def compute_statistics(routes, num_experts=60):\n",
        "    \"\"\"Compute expert usage statistics.\"\"\"\n",
        "    # Count expert selections\n",
        "    expert_counts = Counter()\n",
        "    weighted_counts = Counter()\n",
        "\n",
        "    for route in routes:\n",
        "        for expert_id, weight in zip(route['topk_ids'], route['topk_weights']):\n",
        "            expert_counts[expert_id] += 1\n",
        "            weighted_counts[expert_id] += weight\n",
        "\n",
        "    # Ensure all experts are represented\n",
        "    for i in range(num_experts):\n",
        "        if i not in expert_counts:\n",
        "            expert_counts[i] = 0\n",
        "            weighted_counts[i] = 0.0\n",
        "\n",
        "    # Convert to arrays\n",
        "    experts = list(range(num_experts))\n",
        "    counts = [expert_counts[i] for i in experts]\n",
        "    weights = [weighted_counts[i] for i in experts]\n",
        "\n",
        "    # Normalize\n",
        "    total_selections = sum(counts)\n",
        "    normalized = [c / total_selections if total_selections > 0 else 0 for c in counts]\n",
        "\n",
        "    # Compute entropy\n",
        "    probs = np.array(normalized)\n",
        "    probs = probs[probs > 0]  # Remove zeros for log\n",
        "    entropy = -np.sum(probs * np.log2(probs)) if len(probs) > 0 else 0\n",
        "    max_entropy = np.log2(num_experts)\n",
        "    normalized_entropy = entropy / max_entropy\n",
        "\n",
        "    # Top-K experts\n",
        "    top_3 = sorted(expert_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "    return {\n",
        "        'experts': experts,\n",
        "        'counts': counts,\n",
        "        'normalized': normalized,\n",
        "        'weighted': weights,\n",
        "        'entropy': entropy,\n",
        "        'max_entropy': max_entropy,\n",
        "        'normalized_entropy': normalized_entropy,\n",
        "        'top_3': top_3,\n",
        "        'total_tokens': len(routes),\n",
        "        'total_selections': total_selections,\n",
        "        'num_experts': num_experts\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_histogram(stats, metadata, output_path='expert_hist.png'):\n",
        "    \"\"\"Generate and save expert usage histogram.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "    # Color bars by usage (hot = more used)\n",
        "    colors = plt.cm.RdYlBu_r(np.array(stats['normalized']) / max(stats['normalized']) if max(stats['normalized']) > 0 else np.zeros(len(stats['normalized'])))\n",
        "\n",
        "    # Plot 1: Raw counts\n",
        "    ax1 = axes[0]\n",
        "    bars1 = ax1.bar(stats['experts'], stats['counts'], color=colors, edgecolor='black', linewidth=0.5)\n",
        "    ax1.set_xlabel('Expert ID', fontsize=12)\n",
        "    ax1.set_ylabel('Selection Count', fontsize=12)\n",
        "    ax1.set_title(f'MoE Expert Usage Distribution (Layer 0)\\n{metadata[\"model_id\"]}', fontsize=14)\n",
        "    ax1.set_xlim(-1, stats['num_experts'])\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Highlight top 3\n",
        "    for expert_id, count in stats['top_3']:\n",
        "        ax1.annotate(f'#{expert_id}\\n({count})',\n",
        "                    xy=(expert_id, count),\n",
        "                    xytext=(expert_id, count + max(stats['counts'])*0.05),\n",
        "                    ha='center', fontsize=9, fontweight='bold', color='red')\n",
        "\n",
        "    # Plot 2: Normalized distribution\n",
        "    ax2 = axes[1]\n",
        "    bars2 = ax2.bar(stats['experts'], stats['normalized'], color=colors, edgecolor='black', linewidth=0.5)\n",
        "    ax2.axhline(y=1/stats['num_experts'], color='red', linestyle='--', linewidth=2, label=f'Uniform ({1/stats[\"num_experts\"]:.4f})')\n",
        "    ax2.set_xlabel('Expert ID', fontsize=12)\n",
        "    ax2.set_ylabel('Selection Probability', fontsize=12)\n",
        "    ax2.set_title('Normalized Expert Selection Distribution', fontsize=14)\n",
        "    ax2.set_xlim(-1, stats['num_experts'])\n",
        "    ax2.legend(loc='upper right')\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add statistics text box\n",
        "    stats_text = (\n",
        "        f\"Total Tokens: {stats['total_tokens']}\\n\"\n",
        "        f\"Total Selections: {stats['total_selections']}\\n\"\n",
        "        f\"Top-K per token: {metadata.get('top_k', 4)}\\n\"\n",
        "        f\"Entropy: {stats['entropy']:.3f} bits\\n\"\n",
        "        f\"Normalized Entropy: {stats['normalized_entropy']:.3f}\\n\"\n",
        "        f\"Top-3 Experts: {', '.join([f'#{e}({c})' for e,c in stats['top_3']])}\"\n",
        "    )\n",
        "\n",
        "    fig.text(0.02, 0.02, stats_text, fontsize=10, family='monospace',\n",
        "             verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(bottom=0.15)\n",
        "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Histogram saved to {output_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    jsonl_path = sys.argv[1] if len(sys.argv) > 1 else 'moe_routes.jsonl'\n",
        "    output_path = sys.argv[2] if len(sys.argv) > 2 else 'expert_hist.png'\n",
        "\n",
        "    print(f\"Loading routing data from {jsonl_path}...\")\n",
        "    metadata, routes = load_routing_data(jsonl_path)\n",
        "\n",
        "    if not routes:\n",
        "        print(\"Error: No routing records found!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loaded {len(routes)} routing records\")\n",
        "    print(f\"Model: {metadata.get('model_id', 'Unknown')}\")\n",
        "    print(f\"Device: {metadata.get('device', 'Unknown')}\")\n",
        "\n",
        "    # Determine number of experts (Qwen1.5-MoE has 60 experts)\n",
        "    num_experts = 60\n",
        "\n",
        "    print(\"\\nComputing statistics...\")\n",
        "    stats = compute_statistics(routes, num_experts)\n",
        "\n",
        "    print(f\"\\n=== Expert Usage Analysis ===\")\n",
        "    print(f\"Total tokens processed: {stats['total_tokens']}\")\n",
        "    print(f\"Total expert selections: {stats['total_selections']}\")\n",
        "    print(f\"\\nTop-3 Most Used Experts:\")\n",
        "    for rank, (expert_id, count) in enumerate(stats['top_3'], 1):\n",
        "        pct = count / stats['total_selections'] * 100\n",
        "        print(f\"  {rank}. Expert #{expert_id}: {count} selections ({pct:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nEntropy Analysis:\")\n",
        "    print(f\"  Entropy: {stats['entropy']:.3f} bits\")\n",
        "    print(f\"  Max Entropy (uniform): {stats['max_entropy']:.3f} bits\")\n",
        "    print(f\"  Normalized Entropy: {stats['normalized_entropy']:.3f}\")\n",
        "\n",
        "    if stats['normalized_entropy'] > 0.9:\n",
        "        interpretation = \"Expert usage is highly uniform - good load balancing.\"\n",
        "    elif stats['normalized_entropy'] > 0.7:\n",
        "        interpretation = \"Expert usage is moderately balanced with some specialization.\"\n",
        "    else:\n",
        "        interpretation = \"Expert usage is concentrated - some experts dominate.\"\n",
        "\n",
        "    print(f\"  Interpretation: {interpretation}\")\n",
        "\n",
        "    print(f\"\\nGenerating histogram...\")\n",
        "    plot_histogram(stats, metadata, output_path)\n",
        "\n",
        "    # Save analysis to JSON\n",
        "    analysis = {\n",
        "        'total_tokens': stats['total_tokens'],\n",
        "        'total_selections': stats['total_selections'],\n",
        "        'top_3_experts': [{'expert_id': e, 'count': c, 'percentage': c/stats['total_selections']*100} for e, c in stats['top_3']],\n",
        "        'entropy_bits': stats['entropy'],\n",
        "        'max_entropy_bits': stats['max_entropy'],\n",
        "        'normalized_entropy': stats['normalized_entropy'],\n",
        "        'interpretation': interpretation\n",
        "    }\n",
        "\n",
        "    with open('analysis.json', 'w') as f:\n",
        "        json.dump(analysis, f, indent=2)\n",
        "    print(\"Analysis saved to analysis.json\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa7eb643",
      "metadata": {
        "id": "fa7eb643"
      },
      "source": [
        "## Step 7: Run Generation WITHOUT Logging (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "849e34be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "849e34be",
        "outputId": "d6ee99e3-0afc-41a3-a6ee-72cb280c17f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-20 07:12:10.352774: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768893130.384613   23575 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768893130.395639   23575 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768893130.429346   23575 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768893130.429380   23575 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768893130.429384   23575 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768893130.429388   23575 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "MoE logging enabled, output: moe_routes.jsonl\n",
            "MoE logging patch applied successfully!\n",
            "Loading prompts...\n",
            "Loaded 25 prompts\n",
            "Initializing vLLM with allenai/OLMoE-1B-7B-0924...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "WARNING 01-20 07:12:22 config.py:2276] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 01-20 07:12:44 config.py:510] This model supports multiple tasks: {'classify', 'embed', 'reward', 'score', 'generate'}. Defaulting to 'generate'.\n",
            "WARNING 01-20 07:12:44 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "WARNING 01-20 07:12:44 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
            "INFO 01-20 07:12:44 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='allenai/OLMoE-1B-7B-0924', speculative_config=None, tokenizer='allenai/OLMoE-1B-7B-0924', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMoE-1B-7B-0924, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
            "INFO 01-20 07:12:45 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 01-20 07:12:45 selector.py:129] Using XFormers backend.\n",
            "INFO 01-20 07:12:46 model_runner.py:1094] Starting to load model allenai/OLMoE-1B-7B-0924...\n",
            "INFO 01-20 07:12:46 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 3/3 [00:56<00:00, 18.90s/it]\n",
            "INFO 01-20 07:13:44 model_runner.py:1099] Loading model weights took 12.8889 GB\n",
            "[MoE Logger] Initialized. Logging layer 0 to moe_routes.jsonl\n",
            "WARNING 01-20 07:13:44 fused_moe.py:374] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=1024,device_name=Tesla_T4.json\n",
            "INFO 01-20 07:13:48 worker.py:241] Memory profiling takes 4.01 seconds\n",
            "INFO 01-20 07:13:48 worker.py:241] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.98) = 14.45GiB\n",
            "INFO 01-20 07:13:48 worker.py:241] model weights take 12.89GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 1.01GiB.\n",
            "INFO 01-20 07:13:48 gpu_executor.py:76] # GPU blocks: 517, # CPU blocks: 2048\n",
            "INFO 01-20 07:13:48 gpu_executor.py:80] Maximum concurrency for 512 tokens per request: 16.16x\n",
            "INFO 01-20 07:13:51 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 7.03 seconds\n",
            "Generating...\n",
            "Processed prompts: 100% 25/25 [00:08<00:00,  3.05it/s, est. speed input: 167.85 toks/s, output: 380.18 toks/s]\n",
            "\n",
            "Generation complete!\n",
            "Time: 8.23s\n",
            "Tokens: 3119\n",
            "Tokens/sec: 378.94\n",
            "Timing data saved to timing.json\n",
            "[MoE Logger] Closed. Logged 0 token routings.\n",
            "MoE routing log saved to moe_routes.jsonl\n",
            "\n",
            "=== Sample Outputs ===\n",
            "\n",
            "Prompt 1: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning an...\n",
            "Output: \n",
            "\n",
            "A. $8\n",
            "B. $12\n",
            "C. $16\n",
            "D. $20\n",
            "\n",
            "Answer: C. $16\n",
            "\n",
            "The farmer sells 16 eggs per day at $2 per egg. She sells four dozen eggs per week at $16 per dozen. She...\n",
            "\n",
            "Prompt 2: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bol...\n",
            "Output: \n",
            "\n",
            "A.  2\n",
            "B.  3\n",
            "C.  4\n",
            "D.  5\n",
            "\n",
            "Answer:  C\n",
            "\n",
            "The correct answer is C.  The total number of bolts is 4.  The first bolt of blue fiber is used to make the rob...\n",
            "\n",
            "Prompt 3: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts...\n",
            "Output: \n",
            "\n",
            "A. $20,000\n",
            "B. $30,000\n",
            "C. $40,000\n",
            "D. $50,000\n",
            "\n",
            "The correct answer is C.\n",
            "\n",
            "The formula for profit is:\n",
            "\n",
            "Profit = Selling Price - Cost\n",
            "\n",
            "In this case, the ...\n",
            "[rank0]:[W120 07:14:00.167892891 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ],
      "source": [
        "# First run: WITHOUT logging (baseline timing)\n",
        "!python run_generate.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "74ccfac7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ccfac7",
        "outputId": "3d90ddf5-6f11-443b-ab20-4cd1cc11fe8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"no_log\": {\n",
            "    \"wall_time_sec\": 10.592206478118896,\n",
            "    \"tokens_generated\": 3119,\n",
            "    \"tokens_per_sec\": 294.4617824853725\n",
            "  },\n",
            "  \"log\": {\n",
            "    \"wall_time_sec\": 8.230786323547363,\n",
            "    \"tokens_generated\": 3119,\n",
            "    \"tokens_per_sec\": 378.9431382852071\n",
            "  }\n",
            "}"
          ]
        }
      ],
      "source": [
        "# Check timing results\n",
        "!cat timing.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ea3ee7",
      "metadata": {
        "id": "20ea3ee7"
      },
      "source": [
        "## Step 8: Run Generation WITH Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "357f1f3a",
      "metadata": {
        "id": "357f1f3a"
      },
      "outputs": [],
      "source": [
        "# Reset the logger singleton for clean run\n",
        "from moe_logger import reset_moe_logger\n",
        "reset_moe_logger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "b396152f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b396152f",
        "outputId": "79beebdc-583c-4d60-c1f4-53a71c36087e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-20 07:14:51.818496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768893291.838186   24297 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768893291.844218   24297 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768893291.859751   24297 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768893291.859776   24297 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768893291.859782   24297 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768893291.859786   24297 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "MoE logging enabled, output: moe_routes.jsonl\n",
            "MoE logging patch applied successfully!\n",
            "Loading prompts...\n",
            "Loaded 25 prompts\n",
            "Initializing vLLM with allenai/OLMoE-1B-7B-0924...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "WARNING 01-20 07:15:05 config.py:2276] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 01-20 07:15:27 config.py:510] This model supports multiple tasks: {'embed', 'classify', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
            "WARNING 01-20 07:15:27 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "WARNING 01-20 07:15:27 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
            "INFO 01-20 07:15:27 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='allenai/OLMoE-1B-7B-0924', speculative_config=None, tokenizer='allenai/OLMoE-1B-7B-0924', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/OLMoE-1B-7B-0924, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
            "INFO 01-20 07:15:28 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 01-20 07:15:28 selector.py:129] Using XFormers backend.\n",
            "INFO 01-20 07:15:29 model_runner.py:1094] Starting to load model allenai/OLMoE-1B-7B-0924...\n",
            "INFO 01-20 07:15:29 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
            "Loading safetensors checkpoint shards: 100% 3/3 [00:57<00:00, 19.03s/it]\n",
            "INFO 01-20 07:16:27 model_runner.py:1099] Loading model weights took 12.8889 GB\n",
            "[MoE Logger] Initialized. Logging layer 0 to moe_routes.jsonl\n",
            "WARNING 01-20 07:16:27 fused_moe.py:374] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=64,N=1024,device_name=Tesla_T4.json\n",
            "INFO 01-20 07:16:30 worker.py:241] Memory profiling takes 3.46 seconds\n",
            "INFO 01-20 07:16:30 worker.py:241] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.98) = 14.45GiB\n",
            "INFO 01-20 07:16:30 worker.py:241] model weights take 12.89GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 1.01GiB.\n",
            "INFO 01-20 07:16:31 gpu_executor.py:76] # GPU blocks: 517, # CPU blocks: 2048\n",
            "INFO 01-20 07:16:31 gpu_executor.py:80] Maximum concurrency for 512 tokens per request: 16.16x\n",
            "INFO 01-20 07:16:34 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 6.87 seconds\n",
            "Generating...\n",
            "Processed prompts: 100% 25/25 [00:08<00:00,  2.94it/s, est. speed input: 161.71 toks/s, output: 366.28 toks/s]\n",
            "\n",
            "Generation complete!\n",
            "Time: 8.54s\n",
            "Tokens: 3119\n",
            "Tokens/sec: 365.22\n",
            "Timing data saved to timing.json\n",
            "[MoE Logger] Closed. Logged 0 token routings.\n",
            "MoE routing log saved to moe_routes.jsonl\n",
            "\n",
            "=== Sample Outputs ===\n",
            "\n",
            "Prompt 1: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning an...\n",
            "Output: \n",
            "\n",
            "A. $8\n",
            "B. $12\n",
            "C. $16\n",
            "D. $20\n",
            "\n",
            "Answer: C. $16\n",
            "\n",
            "The farmer sells 16 eggs per day at $2 per egg. She sells four dozen eggs per week at $16 per dozen. She...\n",
            "\n",
            "Prompt 2: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bol...\n",
            "Output: \n",
            "\n",
            "A.  2\n",
            "B.  3\n",
            "C.  4\n",
            "D.  5\n",
            "\n",
            "Answer:  C\n",
            "\n",
            "The correct answer is C.  The total number of bolts is 4.  The first bolt of blue fiber is used to make the rob...\n",
            "\n",
            "Prompt 3: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts...\n",
            "Output: \n",
            "\n",
            "A. $20,000\n",
            "B. $30,000\n",
            "C. $40,000\n",
            "D. $50,000\n",
            "\n",
            "The correct answer is C.\n",
            "\n",
            "The formula for profit is:\n",
            "\n",
            "Profit = Selling Price - Cost\n",
            "\n",
            "In this case, the ...\n",
            "[rank0]:[W120 07:16:44.788881422 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
          ]
        }
      ],
      "source": [
        "# Second run: WITH logging enabled\n",
        "import os\n",
        "os.environ['VLLM_LOG_MOE'] = 'moe_routes.jsonl'\n",
        "\n",
        "# Need to run in subprocess to pick up env var before import\n",
        "!VLLM_LOG_MOE=moe_routes.jsonl python run_generate.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e79ba6dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e79ba6dd",
        "outputId": "617e987e-2e52-467b-fdb4-b8b62c1d80be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"no_log\": {\n",
            "    \"wall_time_sec\": 10.592206478118896,\n",
            "    \"tokens_generated\": 3119,\n",
            "    \"tokens_per_sec\": 294.4617824853725\n",
            "  },\n",
            "  \"log\": {\n",
            "    \"wall_time_sec\": 8.540061473846436,\n",
            "    \"tokens_generated\": 3119,\n",
            "    \"tokens_per_sec\": 365.21985345794064\n",
            "  }\n",
            "}"
          ]
        }
      ],
      "source": [
        "# Check timing results with both runs\n",
        "!cat timing.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "41b90e78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41b90e78",
        "outputId": "d9f6356a-76e7-49c4-af3a-4676f78e80df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"type\": \"meta\", \"model_id\": \"allenai/OLMoE-1B-7B-0924\", \"vllm_version\": \"0.6.6.post1\", \"torch_version\": \"2.5.1+cu124\", \"device\": \"Tesla T4\", \"seed\": 1234, \"layers_logged\": [0], \"top_k\": 8, \"num_experts\": 64}\n"
          ]
        }
      ],
      "source": [
        "# Check the log file\n",
        "!head -20 moe_routes.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "726e9758",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "726e9758",
        "outputId": "a0ef69ca-0382-49a6-87f0-6f22a925f912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 moe_routes.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Count records\n",
        "!wc -l moe_routes.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74c086c",
      "metadata": {
        "id": "b74c086c"
      },
      "source": [
        "## Step 9: Generate Expert Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "39f1cc62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39f1cc62",
        "outputId": "5b3590bc-cdf7-4e88-be05-22f5e06822cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading routing data from moe_routes.jsonl...\n",
            "Error: No routing records found!\n"
          ]
        }
      ],
      "source": [
        "# Generate the histogram\n",
        "!python plot_expert_histogram.py moe_routes.jsonl expert_hist.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "456b1718",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "456b1718",
        "outputId": "e92dc3c6-b80d-4829-d10c-514902a0d6ef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'expert_hist.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1667397531.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display the histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'expert_hist.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'expert_hist.png'"
          ]
        }
      ],
      "source": [
        "# Display the histogram\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename='expert_hist.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5043a6f1",
      "metadata": {
        "id": "5043a6f1"
      },
      "outputs": [],
      "source": [
        "# Show analysis\n",
        "!cat analysis.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4275c5e",
      "metadata": {
        "id": "f4275c5e"
      },
      "source": [
        "## Step 10: Download All Deliverables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e17f0ae",
      "metadata": {
        "id": "1e17f0ae"
      },
      "outputs": [],
      "source": [
        "# Create a zip file with all deliverables\n",
        "!zip -r deliverables.zip \\\n",
        "    moe_logger.py \\\n",
        "    vllm_moe_patch.py \\\n",
        "    make_prompts.py \\\n",
        "    run_generate.py \\\n",
        "    plot_expert_histogram.py \\\n",
        "    prompts.txt \\\n",
        "    moe_routes.jsonl \\\n",
        "    expert_hist.png \\\n",
        "    timing.json \\\n",
        "    analysis.json\n",
        "\n",
        "print(\"\\n=== Deliverables Package Created ===\")\n",
        "!unzip -l deliverables.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550bd520",
      "metadata": {
        "id": "550bd520"
      },
      "outputs": [],
      "source": [
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('deliverables.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138900ec",
      "metadata": {
        "id": "138900ec"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "1. ✅ Installed vLLM with precompiled kernels\n",
        "2. ✅ Created MoE logging patch that hooks into FusedMoE\n",
        "3. ✅ Generated prompts from GSM8K (25 questions)\n",
        "4. ✅ Run inference without logging (baseline)\n",
        "5. ✅ Run inference with logging (to measure overhead)\n",
        "6. ✅ Generated expert usage histogram\n",
        "7. ✅ Created all required deliverables\n",
        "\n",
        "Download `deliverables.zip` to get all files for your submission."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}